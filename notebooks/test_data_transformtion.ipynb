{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import torch, mne\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import numpy as np\n",
    "\n",
    "from collections import OrderedDict\n",
    "_EXTRA_CHANNELS = 5\n",
    "\n",
    "from mne.io.constants import FIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LEFT_NUMBERS = list(reversed(range(1, 9, 2)))\n",
    "_RIGHT_NUMBERS = list(range(2, 10, 2))\n",
    "\n",
    "\n",
    "DEEP_1010_CHS_LISTING = [\n",
    "    # EEG\n",
    "    \"NZ\",\n",
    "    \"FP1\", \"FPZ\", \"FP2\",\n",
    "    \"AF7\", \"AF3\", \"AFZ\", \"AF4\", \"AF8\",\n",
    "    \"F9\", *[\"F{}\".format(n) for n in _LEFT_NUMBERS], \"FZ\", *[\"F{}\".format(n) for n in _RIGHT_NUMBERS], \"F10\",\n",
    "\n",
    "    \"FT9\", \"FT7\", *[\"FC{}\".format(n) for n in _LEFT_NUMBERS[1:]], \"FCZ\",\n",
    "    *[\"FC{}\".format(n) for n in _RIGHT_NUMBERS[:-1]], \"FT8\", \"FT10\",\n",
    "                                                                                                                                  \n",
    "    \"T9\", \"T7\", \"T3\",  *[\"C{}\".format(n) for n in _LEFT_NUMBERS[1:]], \"CZ\",\n",
    "    *[\"C{}\".format(n) for n in _RIGHT_NUMBERS[:-1]], \"T4\", \"T8\", \"T10\",\n",
    "\n",
    "    \"TP9\", \"TP7\", *[\"CP{}\".format(n) for n in _LEFT_NUMBERS[1:]], \"CPZ\",\n",
    "    *[\"CP{}\".format(n) for n in _RIGHT_NUMBERS[:-1]], \"TP8\", \"TP10\",\n",
    "\n",
    "    \"P9\", \"P7\", \"T5\",  *[\"P{}\".format(n) for n in _LEFT_NUMBERS[1:]], \"PZ\",\n",
    "    *[\"P{}\".format(n) for n in _RIGHT_NUMBERS[:-1]],  \"T6\", \"P8\", \"P10\",\n",
    "\n",
    "    \"PO7\", \"PO3\", \"POZ\", \"PO4\", \"PO8\",\n",
    "    \"O1\",  \"OZ\", \"O2\",\n",
    "    \"IZ\",\n",
    "    # EOG\n",
    "    \"VEOGL\", \"VEOGR\", \"HEOGL\", \"HEOGR\",\n",
    "\n",
    "    # Ear clip references\n",
    "    \"A1\", \"A2\", \"REF\",\n",
    "    # SCALING\n",
    "    \"SCALE\",\n",
    "    # Extra\n",
    "    *[\"EX{}\".format(n) for n in range(1, _EXTRA_CHANNELS+1)]\n",
    "]\n",
    "\n",
    "\n",
    "# Not crazy about this approach..\n",
    "from mne.utils._bunch import NamedInt\n",
    "from mne.io.constants import FIFF\n",
    "# Careful this doesn't overlap with future additions to MNE, might have to coordinate\n",
    "DEEP_1010_SCALE_CH = NamedInt('DN3_DEEP1010_SCALE_CH', 3000)\n",
    "DEEP_1010_EXTRA_CH = NamedInt('DN3_DEEP1010_EXTRA_CH', 3001)\n",
    "\n",
    "EEG_INDS = list(range(0, DEEP_1010_CHS_LISTING.index('VEOGL')))\n",
    "EOG_INDS = [DEEP_1010_CHS_LISTING.index(ch) for ch in [\"VEOGL\", \"VEOGR\", \"HEOGL\", \"HEOGR\"]]\n",
    "REF_INDS = [DEEP_1010_CHS_LISTING.index(ch) for ch in [\"A1\", \"A2\", \"REF\"]]\n",
    "EXTRA_INDS = list(range(len(DEEP_1010_CHS_LISTING) - _EXTRA_CHANNELS, len(DEEP_1010_CHS_LISTING)))\n",
    "SCALE_IND = -len(EXTRA_INDS) + len(DEEP_1010_CHS_LISTING)\n",
    "_NUM_EEG_CHS = len(DEEP_1010_CHS_LISTING) - len(EOG_INDS) - len(REF_INDS) - len(EXTRA_INDS) - 1\n",
    "\n",
    "DEEP_1010_CH_TYPES = ([FIFF.FIFFV_EEG_CH] * _NUM_EEG_CHS) + ([FIFF.FIFFV_EOG_CH] * len(EOG_INDS)) + \\\n",
    "                     ([FIFF.FIFFV_EEG_CH] * len(REF_INDS)) + [DEEP_1010_SCALE_CH] + \\\n",
    "                     ([DEEP_1010_EXTRA_CH] * _EXTRA_CHANNELS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Torch Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    Base class for various preprocessing actions. Sub-classes are called with a subclass of `_Recording`\n",
    "    and operate on these instances in-place.\n",
    "\n",
    "    Any modifications to data specifically should be implemented through a subclass of :any:`BaseTransform`, and\n",
    "    returned by the method :meth:`get_transform()`\n",
    "    \"\"\"\n",
    "    def __call__(self, recording, **kwargs):\n",
    "        \"\"\"\n",
    "        Preprocess a particular recording. This is allowed to modify aspects of the recording in-place, but is not\n",
    "        strictly advised.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        recording :\n",
    "        kwargs : dict\n",
    "                 New :any:`_Recording` subclasses may need to provide additional arguments. This is here for support of\n",
    "                 this.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_transform(self):\n",
    "        \"\"\"\n",
    "        Generate and return any transform associated with this preprocessor. Should be used after applying this\n",
    "        to a dataset, i.e. through :meth:`DN3ataset.preprocess`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        transform : BaseTransform\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DN3ataset(TorchDataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Base class for that specifies the interface for DN3 datasets.\n",
    "        \"\"\"\n",
    "        self._transforms = list()\n",
    "        self._safe_mode = False\n",
    "        self._mutli_proc_start = None\n",
    "        self._mutli_proc_end = None\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def sfreq(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        sampling_frequency: float, list\n",
    "                            The sampling frequencies employed by the dataset.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def channels(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        channels: list\n",
    "                  The channel sets used by the dataset.\n",
    "                \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        sequence_length: int, list\n",
    "                         The length of each instance in number of samples\n",
    "            \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def clone(self):\n",
    "        \"\"\"\n",
    "        A copy of this object to allow the repetition of recordings, thinkers, etc. that load data from\n",
    "        the same memory/files but have their own tracking of ids.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cloned : DN3ataset\n",
    "                 New copy of this object.\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def add_transform(self, transform):\n",
    "        \"\"\"\n",
    "        Add a transformation that is applied to every fetched item in the dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        transform : BaseTransform\n",
    "                    For each item retrieved by __getitem__, transform is called to modify that item.\n",
    "        \"\"\"\n",
    "        if isinstance(transform, InstanceTransform):\n",
    "            self._transforms.append(transform)\n",
    "\n",
    "    def _execute_transforms(self, *x):\n",
    "        for transform in self._transforms:\n",
    "            assert isinstance(transform, InstanceTransform)\n",
    "            if transform.only_trial_data:\n",
    "                new_x = transform(x[0])\n",
    "                if isinstance(new_x, (list, tuple)):\n",
    "                    x = (*new_x, *x[1:])\n",
    "                else:\n",
    "                    x = (new_x, *x[1:])\n",
    "            else:\n",
    "                x = transform(*x)\n",
    "\n",
    "            if self._safe_mode:\n",
    "                for i in range(len(x)):\n",
    "                    if torch.any(torch.isnan(x[i])):\n",
    "                        raise DN3atasetNanFound(\"NaN generated by transform {} for {}'th tensor\".format(\n",
    "                            self, i))\n",
    "        return x\n",
    "\n",
    "    def clear_transforms(self):\n",
    "        \"\"\"\n",
    "        Remove all added transforms from dataset.\n",
    "        \"\"\"\n",
    "        self._transforms = list()\n",
    "\n",
    "    def preprocess(self, preprocessor: Preprocessor, apply_transform=True):\n",
    "        \"\"\"\n",
    "        Applies a preprocessor to the dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        preprocessor : Preprocessor\n",
    "                       A preprocessor to be applied\n",
    "        apply_transform : bool\n",
    "                          Whether to apply the transform to this dataset (and all members e.g thinkers or sessions)\n",
    "                          after preprocessing them. Alternatively, the preprocessor is returned for manual application\n",
    "                          of its transform through :meth:`Preprocessor.get_transform()`\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        processed_data : ndarry\n",
    "                         Data that has been modified by the preprocessor, should be in the shape of [*, C, T], with C\n",
    "                         and T according with the `channels` and `sequence_length` properties respectively.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def to_numpy(self, batch_size=64, batch_transforms: list = None, num_workers=4, **dataloader_kwargs):\n",
    "        \"\"\"\n",
    "        Commits the dataset to numpy-formatted arrays. Useful for saving dataset to disk, or preparing for tools that\n",
    "        expect numpy-formatted data rather than iteratable.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        A pytorch :any:`DataLoader` is used to fetch the data to conveniently leverage multiprocessing, and naturally\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "                   The number of items to fetch per worker. This probably doesn't need much tuning.\n",
    "        num_workers: int\n",
    "                     The number of spawned processes to fetch and transform data.\n",
    "        batch_transforms: list\n",
    "                         These are potential batch-level transforms that\n",
    "        dataloader_kwargs: dict\n",
    "                          Keyword arguments for the pytorch :any:`DataLoader` that underpins the fetched data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        data: list\n",
    "              A list of numpy arrays.\n",
    "        \"\"\"\n",
    "        dataloader_kwargs.setdefault('batch_size', batch_size)\n",
    "        dataloader_kwargs.setdefault('num_workers', num_workers)\n",
    "        dataloader_kwargs.setdefault('shuffle', False)\n",
    "        dataloader_kwargs.setdefault('drop_last', False)\n",
    "\n",
    "        batch_transforms = list() if batch_transforms is None else batch_transforms\n",
    "\n",
    "        loaded = None\n",
    "        loader = DataLoader(self, **dataloader_kwargs)\n",
    "        for batch in tqdm.tqdm(loader, desc=\"Loading Batches\"):\n",
    "            for xform in batch_transforms:\n",
    "                assert callable(xform)\n",
    "                batch = xform(batch)\n",
    "            # cpu just to be certain, shouldn't affect things otherwise\n",
    "            batch = [b.cpu().numpy() for b in batch]\n",
    "            if loaded is None:\n",
    "                loaded = batch\n",
    "            else:\n",
    "                loaded = [np.concatenate([loaded[i], batch[i]], axis=0) for i in range(len(batch))]\n",
    "\n",
    "        return loaded\n",
    "    \n",
    "class _Recording(DN3ataset, ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for any supported recording\n",
    "    \"\"\"\n",
    "    def __init__(self, info, session_id, person_id, tlen, ch_ind_picks=None):\n",
    "        super().__init__()\n",
    "        self.info = info\n",
    "        self.picks = ch_ind_picks if ch_ind_picks is not None else list(range(len(info['chs'])))\n",
    "        self._recording_channels = [(ch['ch_name'], int(ch['kind'])) for idx, ch in enumerate(info['chs'])\n",
    "                                    if idx in self.picks]\n",
    "        self._recording_sfreq = info['sfreq']\n",
    "        self._recording_len = int(self._recording_sfreq * tlen)\n",
    "        assert self._recording_sfreq is not None\n",
    "        self.session_id = session_id\n",
    "        self.person_id = person_id\n",
    "\n",
    "    def get_all(self):\n",
    "        all_recordings = [self[i] for i in range(len(self))]\n",
    "        return [torch.stack(t) for t in zip(*all_recordings)]\n",
    "\n",
    "    @property\n",
    "    def sfreq(self):\n",
    "        sfreq = self._recording_sfreq\n",
    "        for xform in self._transforms:\n",
    "            sfreq = xform.new_sfreq(sfreq)\n",
    "        return sfreq\n",
    "\n",
    "    @property\n",
    "    def channels(self):\n",
    "        channels = np.array(self._recording_channels)\n",
    "        for xform in self._transforms:\n",
    "            channels = xform.new_channels(channels)\n",
    "        return channels\n",
    "\n",
    "    @property\n",
    "    def sequence_length(self):\n",
    "        sequence_length = self._recording_len\n",
    "        for xform in self._transforms:\n",
    "            sequence_length = xform.new_sequence_length(sequence_length)\n",
    "        return sequence_length\n",
    "\n",
    "class RawTorchRecording(_Recording):\n",
    "    \"\"\"\n",
    "    Interface for bridging mne Raw instances as PyTorch compatible \"Dataset\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : mne.io.Raw\n",
    "          Raw data, data does not need to be preloaded.\n",
    "    tlen : float\n",
    "          Length of recording specified in seconds.\n",
    "    session_id : (int, str, optional)\n",
    "          A unique (with respect to a thinker within an eventual dataset) identifier for the current recording\n",
    "          session. If not specified, defaults to '0'.\n",
    "    person_id : (int, str, optional)\n",
    "          A unique (with respect to an eventual dataset) identifier for the particular person being recorded.\n",
    "    stride : int\n",
    "          The number of samples to skip between each starting offset of loaded samples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw: mne.io.Raw, tlen, session_id=0, person_id=0, stride=1, ch_ind_picks=None, decimate=1,\n",
    "                 bad_spans=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Interface for bridging mne Raw instances as PyTorch compatible \"Dataset\".\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        raw : mne.io.Raw\n",
    "              Raw data, data does not need to be preloaded.\n",
    "        tlen : float\n",
    "              Length of each retrieved portion of the recording.\n",
    "        session_id : (int, str, optional)\n",
    "              A unique (with respect to a thinker within an eventual dataset) identifier for the current recording\n",
    "              session. If not specified, defaults to '0'.\n",
    "        person_id : (int, str, optional)\n",
    "              A unique (with respect to an eventual dataset) identifier for the particular person being recorded.\n",
    "        stride : int\n",
    "              The number of samples to skip between each starting offset of loaded samples.\n",
    "        ch_ind_picks : list[int]\n",
    "                       A list of channel indices that have been selected for.\n",
    "        decimate : int\n",
    "                   The number of samples to move before taking the next sample, in other words take every decimate'th\n",
    "                   sample.\n",
    "        bad_spans: List[tuple], None\n",
    "                   These are tuples of (start_seconds, end_seconds) of times that should be avoided. Any sequences that\n",
    "                   would overlap with these sections will be excluded.\n",
    "        \"\"\"\n",
    "        super().__init__(raw.info, session_id, person_id, tlen, ch_ind_picks)\n",
    "        self.filename = raw.filenames[0]\n",
    "        self.decimate = int(decimate)\n",
    "        self._recording_sfreq /= self.decimate\n",
    "        self._recording_len = int(tlen * self._recording_sfreq)\n",
    "        self.stride = stride\n",
    "        # Implement my own (rather than mne's) in-memory buffer when there are savings\n",
    "        self._stride_load = self.decimate > 1 and raw.preload\n",
    "        self.max = kwargs.get('max', None)\n",
    "        self.min = kwargs.get('min', 0)\n",
    "        bad_spans = list() if bad_spans is None else bad_spans\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self._decimated_sequence_starts = list(\n",
    "            range(0, raw.n_times // self.decimate - self._recording_len, self.stride)\n",
    "        )\n",
    "        # TODO come back to this inefficient BS\n",
    "        for start, stop in bad_spans:\n",
    "            start = int(self._recording_sfreq * start)\n",
    "            stop = int(stop * self._recording_sfreq)\n",
    "            drop = list()\n",
    "            for i, span_start in enumerate(self._decimated_sequence_starts):\n",
    "                if start <= span_start < stop or start <= span_start + self._recording_len <= stop:\n",
    "                    drop.append(span_start)\n",
    "            for span_start in drop:\n",
    "                self._decimated_sequence_starts.remove(span_start)\n",
    "\n",
    "        # When the stride is greater than the sequence length, preload savings can be found by chopping the\n",
    "        # sequence into subsequences of length: sequence length. Also, if decimating, can significantly reduce memory\n",
    "        # requirements not otherwise addressed with the Raw object.\n",
    "        if self._stride_load and len(self._decimated_sequence_starts) > 0:\n",
    "            x = raw.get_data(self.picks)\n",
    "            # pre-decimate this data for more preload savings (and for the stride factors to be valid)\n",
    "            x = x[:, ::decimate]\n",
    "            self._x = np.empty([x.shape[0], self._recording_len, len(self._decimated_sequence_starts)], dtype=x.dtype)\n",
    "            for i, start in enumerate(self._decimated_sequence_starts):\n",
    "                self._x[..., i] = x[:, start:start + self._recording_len]\n",
    "        else:\n",
    "            self._raw_workaround(raw)\n",
    "\n",
    "    def _raw_workaround(self, raw):\n",
    "        self.raw = raw\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index < 0:\n",
    "            index += len(self)\n",
    "\n",
    "        if self._stride_load:\n",
    "            x = self._x[:, :, index]\n",
    "        else:\n",
    "            start = self._decimated_sequence_starts[index]\n",
    "            x = self.raw.get_data(self.picks, start=start, stop=start + self._recording_len * self.decimate)\n",
    "            if self.decimate > 1:\n",
    "                x = x[:, ::self.decimate]\n",
    "\n",
    "        scale = 1 if self.max is None else (x.max() - x.min()) / (self.max - self.min)\n",
    "        if scale > 1 or np.isnan(scale):\n",
    "            print('Warning: scale exeeding 1')\n",
    "\n",
    "        x = torch.from_numpy(x).float()\n",
    "\n",
    "        if torch.any(torch.isnan(x)):\n",
    "            print(\"Nan found: raw {}, index {}\".format(self.filename, index))\n",
    "            print(\"Replacing with random values with same shape for now...\")\n",
    "            x = torch.rand_like(x)\n",
    "\n",
    "        return self._execute_transforms(x)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._decimated_sequence_starts)\n",
    "\n",
    "    def preprocess(self, preprocessor: Preprocessor, apply_transform=True):\n",
    "        self.raw = preprocessor(recording=self)\n",
    "        if apply_transform:\n",
    "            self.add_transform(preprocessor.get_transform())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _DumbNumbSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _DumbNamespace:\n",
    "    def __init__(self, d: dict):\n",
    "        self._d = d.copy()\n",
    "        for k in d:\n",
    "            if isinstance(d[k], dict):\n",
    "                d[k] = _DumbNamespace(d[k])\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [_DumbNamespace(d[k][i]) if isinstance(d[k][i], dict) else d[k][i] for i in range(len(d[k]))]\n",
    "        self.__dict__.update(d)\n",
    "\n",
    "    def keys(self):\n",
    "        return list(self.__dict__.keys())\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.__dict__[item]\n",
    "\n",
    "    def as_dict(self):\n",
    "        return self._d\n",
    "\n",
    "\n",
    "def _adopt_auxiliaries(obj, remaining):\n",
    "    def namespaceify(v):\n",
    "        if isinstance(v, dict):\n",
    "            return _DumbNamespace(v)\n",
    "        elif isinstance(v, list):\n",
    "            return [namespaceify(v[i]) for i in range(len(v))]\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    obj.__dict__.update({k: namespaceify(v) for k, v in remaining.items()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_named_channels_deep_1010(channel_names: list, EOG=None, ear_ref=None, extra_channels=None):\n",
    "    \"\"\"\n",
    "    Maps channel names to the Deep1010 format, will automatically map EOG and extra channels if they have been\n",
    "    named according to standard convention. Otherwise provide as keyword arguments.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    channel_names : list\n",
    "                   List of channel names from dataset\n",
    "    EOG : list, str\n",
    "         Must be a single channel name, or left and right EOG channels, optionally vertical L/R then horizontal\n",
    "         L/R for four channels.\n",
    "    ear_ref : Optional, str, list\n",
    "               One or two channels to be used as references. If two, should be left and right in that order.\n",
    "    extra_channels : list, None\n",
    "                     Up to 6 extra channels to include. Currently not standardized, but could include ECG, respiration,\n",
    "                     EMG, etc.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapping : torch.Tensor\n",
    "              Mapping matrix from previous channel sequence to Deep1010.\n",
    "    \"\"\"\n",
    "    map = np.zeros((len(channel_names), len(DEEP_1010_CHS_LISTING)))\n",
    "\n",
    "    if isinstance(EOG, str):\n",
    "        EOG = [EOG] * 4\n",
    "    elif len(EOG) == 1:\n",
    "        EOG = EOG * 4\n",
    "    elif EOG is None or len(EOG) == 0:\n",
    "        EOG = []\n",
    "    elif len(EOG) == 2:\n",
    "        EOG = EOG * 2\n",
    "    else:\n",
    "        assert len(EOG) == 4\n",
    "    for eog_map, eog_std in zip(EOG, EOG_INDS):\n",
    "        try:\n",
    "            map[channel_names.index(eog_map), eog_std] = 1.0\n",
    "        except ValueError:\n",
    "            raise ValueError(\"EOG channel {} not found in provided channels.\".format(eog_map))\n",
    "\n",
    "    if isinstance(ear_ref, str):\n",
    "        ear_ref = [ear_ref] * 2\n",
    "    elif ear_ref is None:\n",
    "        ear_ref = []\n",
    "    else:\n",
    "        assert len(ear_ref) <= len(REF_INDS)\n",
    "    for ref_map, ref_std in zip(ear_ref, REF_INDS):\n",
    "        try:\n",
    "            map[channel_names.index(ref_map), ref_std] = 1.0\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Reference channel {} not found in provided channels.\".format(ref_map))\n",
    "\n",
    "    if isinstance(extra_channels, str):\n",
    "        extra_channels = [extra_channels]\n",
    "    elif extra_channels is None:\n",
    "        extra_channels = []\n",
    "    assert len(extra_channels) <= _EXTRA_CHANNELS\n",
    "    for ch, place in zip(extra_channels, EXTRA_INDS):\n",
    "        if ch is not None:\n",
    "            map[channel_names.index(ch), place] = 1.0\n",
    "\n",
    "    return _deep_1010(map, channel_names, EOG, ear_ref, extra_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _likely_eeg_channel(name):\n",
    "    if name is not None:\n",
    "        for ch in DEEP_1010_CHS_LISTING[:_NUM_EEG_CHS]:\n",
    "            if ch in name.upper():\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _deep_1010(map, names, eog, ear_ref, extra):\n",
    "\n",
    "    for i, ch in enumerate(names):\n",
    "        if ch not in eog and ch not in ear_ref and ch not in extra:\n",
    "            try:\n",
    "                map[i, DEEP_1010_CHS_LISTING.index(str(ch).upper())] = 1.0\n",
    "            except ValueError:\n",
    "                print(\"Warning: channel {} not found in standard layout. Skipping...\".format(ch))\n",
    "                continue\n",
    "\n",
    "    # Normalize for when multiple values are mapped to single location\n",
    "    summed = map.sum(axis=0)[np.newaxis, :]\n",
    "    mapping = torch.from_numpy(np.divide(map, summed, out=np.zeros_like(map), where=summed != 0)).float()\n",
    "    mapping.requires_grad_(False)\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def _valid_character_heuristics(name, informative_characters):\n",
    "    possible = ''.join(c for c in name.upper() if c in informative_characters).replace(' ', '')\n",
    "    if possible == \"\":\n",
    "        print(\"Could not use channel {}. Could not resolve its true label, rename first.\".format(name))\n",
    "        return None\n",
    "    return possible\n",
    "\n",
    "\n",
    "def _check_num_and_get_types(type_dict: OrderedDict):\n",
    "    type_lists = list()\n",
    "    for ch_type, max_num in zip(('eog', 'ref'), (len(EOG_INDS), len(REF_INDS))):\n",
    "        channels = [ch_name for ch_name, _type in type_dict.items() if _type == ch_type]\n",
    "\n",
    "        for name in channels[max_num:]:\n",
    "            print(\"Losing assumed {} channel {} because there are too many.\".format(ch_type, name))\n",
    "            type_dict[name] = None\n",
    "        type_lists.append(channels[:max_num])\n",
    "    return type_lists[0], type_lists[1]\n",
    "\n",
    "def _heuristic_eog_resolution(eog_channel_name):\n",
    "    return _valid_character_heuristics(eog_channel_name, \"VHEOGLR\")\n",
    "\n",
    "\n",
    "def _heuristic_ref_resolution(ref_channel_name: str):\n",
    "    ref_channel_name = ref_channel_name.replace('EAR', '')\n",
    "    ref_channel_name = ref_channel_name.replace('REF', '')\n",
    "    if ref_channel_name.find('A1') != -1:\n",
    "        return 'A1'\n",
    "    elif ref_channel_name.find('A2') != -1:\n",
    "        return 'A2'\n",
    "\n",
    "    if ref_channel_name.find('L') != -1:\n",
    "        return 'A1'\n",
    "    elif ref_channel_name.find('R') != -1:\n",
    "        return 'A2'\n",
    "    return \"REF\"\n",
    "\n",
    "\n",
    "def _heuristic_eeg_resolution(eeg_ch_name: str):\n",
    "    eeg_ch_name = eeg_ch_name.upper()\n",
    "    # remove some common garbage\n",
    "    eeg_ch_name = eeg_ch_name.replace('EEG', '')\n",
    "    eeg_ch_name = eeg_ch_name.replace('REF', '')\n",
    "    informative_characters = set([c for name in DEEP_1010_CHS_LISTING[:_NUM_EEG_CHS] for c in name])\n",
    "    return _valid_character_heuristics(eeg_ch_name, informative_characters)\n",
    "\n",
    "\n",
    "def _heuristic_resolution(old_type_dict: OrderedDict):\n",
    "    resolver = {'eeg': _heuristic_eeg_resolution, 'eog': _heuristic_eog_resolution, 'ref': _heuristic_ref_resolution,\n",
    "                'extra': lambda x: x, None: lambda x: x}\n",
    "\n",
    "    new_type_dict = OrderedDict()\n",
    "\n",
    "    for old_name, ch_type in old_type_dict.items():\n",
    "        if ch_type is None:\n",
    "            new_type_dict[old_name] = None\n",
    "            continue\n",
    "\n",
    "        new_name = resolver[ch_type](old_name)\n",
    "        if new_name is None:\n",
    "            new_type_dict[old_name] = None\n",
    "        else:\n",
    "            while new_name in new_type_dict.keys():\n",
    "                print('Deep1010 Heuristics resulted in duplicate entries for {}, incrementing name, but will be lost '\n",
    "                      'in mapping'.format(new_name))\n",
    "                new_name = new_name + '-COPY'\n",
    "            new_type_dict[new_name] = old_type_dict[old_name]\n",
    "\n",
    "    assert len(new_type_dict) == len(old_type_dict)\n",
    "    return new_type_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_dataset_channels_deep_1010(channels: np.ndarray, exclude_stim=True):\n",
    "    \"\"\"\n",
    "    Maps channels as stored by a :any:`DN3ataset` to the Deep1010 format, will automatically map EOG and extra channels\n",
    "    by type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    channels : np.ndarray\n",
    "               Channels that remain a 1D sequence (they should not have been projected into 2 or 3D grids) of name and\n",
    "               type. This means the array has 2 dimensions:\n",
    "               ..math:: N_{channels} \\by 2\n",
    "               With the latter dimension containing name and type respectively, as is constructed by default in most\n",
    "               cases.\n",
    "    exclude_stim : bool\n",
    "                   This option allows the stim channel to be added as an *extra* channel. The default (True) will not do\n",
    "                   this, and it is very rare if ever where this would be needed.\n",
    "\n",
    "    Warnings\n",
    "    --------\n",
    "    If for some reason the stim channel is labelled with a label from the `DEEP_1010_CHS_LISTING` it will be included\n",
    "    in that location and result in labels bleeding into the observed data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mapping : torch.Tensor\n",
    "              Mapping matrix from previous channel sequence to Deep1010.\n",
    "    \"\"\"\n",
    "    if len(channels.shape) != 2 or channels.shape[1] != 2:\n",
    "        raise ValueError(\"Deep1010 Mapping: channels must be a 2 dimensional array with dim0 = num_channels, dim1 = 2.\"\n",
    "                         \" Got {}\".format(channels.shape))\n",
    "    channel_types = OrderedDict()\n",
    "\n",
    "    # Use this for some semblance of order in the \"extras\"\n",
    "    extra = [None for _ in range(_EXTRA_CHANNELS)]\n",
    "    extra_idx = 0\n",
    "\n",
    "    for name, ch_type in channels:\n",
    "        # Annoyingly numpy converts them to strings...\n",
    "        ch_type = int(ch_type)\n",
    "        if ch_type == FIFF.FIFFV_EEG_CH and _likely_eeg_channel(name):\n",
    "            channel_types[name] = 'eeg'\n",
    "        elif ch_type == FIFF.FIFFV_EOG_CH or name in [DEEP_1010_CHS_LISTING[idx] for idx in EOG_INDS]:\n",
    "            channel_types[name] = 'eog'\n",
    "        elif ch_type == FIFF.FIFFV_STIM_CH:\n",
    "            if exclude_stim:\n",
    "                channel_types[name] = None\n",
    "                continue\n",
    "            # if stim, always set as last extra\n",
    "            channel_types[name] = 'extra'\n",
    "            extra[-1] = name\n",
    "        elif 'REF' in name.upper() or 'A1' in name.upper() or 'A2' in name.upper() or 'EAR' in name.upper():\n",
    "            channel_types[name] = 'ref'\n",
    "        else:\n",
    "            if extra_idx == _EXTRA_CHANNELS - 1 and not exclude_stim:\n",
    "                print(\"Stim channel overwritten by {} in Deep1010 mapping.\".format(name))\n",
    "            elif extra_idx == _EXTRA_CHANNELS:\n",
    "                print(\"No more room in extra channels for {}\".format(name))\n",
    "                continue\n",
    "            channel_types[name] = 'extra'\n",
    "            extra[extra_idx] = name\n",
    "            extra_idx += 1\n",
    "\n",
    "    revised_channel_types = _heuristic_resolution(channel_types)\n",
    "    eog, ref = _check_num_and_get_types(revised_channel_types)\n",
    "\n",
    "    return map_named_channels_deep_1010(list(revised_channel_types.keys()), eog, ref, extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceTransform(object):\n",
    "\n",
    "    def __init__(self, only_trial_data=True):\n",
    "        \"\"\"\n",
    "        Trial transforms are, for the most part, simply operations that are performed on the loaded tensors when they are\n",
    "        fetched via the :meth:`__call__` method. Ideally this is implemented with pytorch operations for ease of execution\n",
    "        graph integration.\n",
    "        \"\"\"\n",
    "        self.only_trial_data = only_trial_data\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n",
    "\n",
    "    def __call__(self, *x):\n",
    "        \"\"\"\n",
    "        Modifies a batch of tensors.\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor, tuple\n",
    "            The trial tensor, not including a batch-dimension. If initialized with `only_trial_data=False`, then this\n",
    "            is a tuple of all ids, labels, etc. being propagated.\n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.Tensor, tuple\n",
    "            The modified trial tensor, or tensors if not `only_trial_data`\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def new_channels(self, old_channels):\n",
    "        \"\"\"\n",
    "        This is an optional method that indicates the transformation modifies the representation and/or presence of\n",
    "        channels.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        old_channels : ndarray\n",
    "                       An array whose last two dimensions are channel names and channel types.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_channels : ndarray\n",
    "                      An array with the channel names and types after this transformation. Supports the addition of\n",
    "                      dimensions e.g. a list of channels into a rectangular grid, but the *final two dimensions* must\n",
    "                      remain the channel names, and types respectively.\n",
    "        \"\"\"\n",
    "        return old_channels\n",
    "\n",
    "    def new_sfreq(self, old_sfreq):\n",
    "        \"\"\"\n",
    "        This is an optional method that indicates the transformation modifies the sampling frequency of the underlying\n",
    "        time-series.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        old_sfreq : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_sfreq : float\n",
    "        \"\"\"\n",
    "        return old_sfreq\n",
    "\n",
    "    def new_sequence_length(self, old_sequence_length):\n",
    "        \"\"\"\n",
    "        This is an optional method that indicates the transformation modifies the length of the acquired extracts,\n",
    "        specified in number of samples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        old_sequence_length : int\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        new_sequence_length : int\n",
    "        \"\"\"\n",
    "        return old_sequence_length\n",
    "    \n",
    "\n",
    "def min_max_normalize(x: torch.Tensor, low=-1, high=1):\n",
    "    if len(x.shape) == 2:\n",
    "        xmin = x.min()\n",
    "        xmax = x.max()\n",
    "        if xmax - xmin == 0:\n",
    "            x = 0\n",
    "            return x\n",
    "    elif len(x.shape) == 3:\n",
    "        xmin = torch.min(torch.min(x, keepdim=True, dim=1)[0], keepdim=True, dim=-1)[0]\n",
    "        xmax = torch.max(torch.max(x, keepdim=True, dim=1)[0], keepdim=True, dim=-1)[0]\n",
    "        constant_trials = (xmax - xmin) == 0\n",
    "        if torch.any(constant_trials):\n",
    "            # If normalizing multiple trials, stabilize the normalization\n",
    "            xmax[constant_trials] = xmax[constant_trials] + 1e-6\n",
    "\n",
    "    x = (x - xmin) / (xmax - xmin)\n",
    "\n",
    "    # Now all scaled 0 -> 1, remove 0.5 bias\n",
    "    x -= 0.5\n",
    "    # Adjust for low/high bias and scale up\n",
    "    x += (high + low) / 2\n",
    "    return (high - low) * x\n",
    "\n",
    "    \n",
    "\n",
    "class MappingDeep1010(InstanceTransform):\n",
    "    \"\"\"\n",
    "    Maps various channel sets into the Deep10-10 scheme, and normalizes data between [-1, 1] with an additional scaling\n",
    "    parameter to describe the relative scale of a trial with respect to the entire dataset.\n",
    "\n",
    "    See https://doi.org/10.1101/2020.12.17.423197  for description.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, max_scale=None, return_mask=False):\n",
    "        \"\"\"\n",
    "        Creates a Deep10-10 mapping for the provided dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "\n",
    "        max_scale : float\n",
    "                    If specified, the scale ind is filled with the relative scale of the trial with respect\n",
    "                    to this, otherwise uses dataset.info.data_max - dataset.info.data_min.\n",
    "        return_mask : bool\n",
    "                      If `True` (`False` by default), an additional tensor is returned after this transform that\n",
    "                      says which channels of the mapping are in fact in use.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.mapping = map_dataset_channels_deep_1010(dataset.channels)\n",
    "        if max_scale is not None:\n",
    "            self.max_scale = max_scale\n",
    "        elif dataset.info is None or dataset.info.data_max is None or dataset.info.data_min is None:\n",
    "            print(f\"Warning: Did not find data scale information for {dataset}\")\n",
    "            self.max_scale = None\n",
    "            pass\n",
    "        else:\n",
    "            self.max_scale = dataset.info.data_max - dataset.info.data_min\n",
    "        self.return_mask = return_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def channel_listing():\n",
    "        return DEEP_1010_CHS_LISTING\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.max_scale is not None:\n",
    "            scale = 2 * (torch.clamp_max((x.max() - x.min()) / self.max_scale, 1.0) - 0.5)\n",
    "        else:\n",
    "            scale = 0\n",
    "\n",
    "        x = (x.transpose(1, 0) @ self.mapping).transpose(1, 0)\n",
    "\n",
    "        for ch_type_inds in (EEG_INDS, EOG_INDS, REF_INDS, EXTRA_INDS):\n",
    "            x[ch_type_inds, :] = min_max_normalize(x[ch_type_inds, :])\n",
    "\n",
    "        used_channel_mask = self.mapping.sum(dim=0).bool()\n",
    "        x[~used_channel_mask, :] = 0\n",
    "\n",
    "        x[SCALE_IND, :] = scale\n",
    "\n",
    "        if self.return_mask:\n",
    "            return (x, used_channel_mask)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def new_channels(self, old_channels: np.ndarray):\n",
    "        channels = list()\n",
    "        for row in range(self.mapping.shape[1]):\n",
    "            active = self.mapping[:, row].nonzero().numpy()\n",
    "            if len(active) > 0:\n",
    "                channels.append(\"-\".join([old_channels[i.item(), 0] for i in active]))\n",
    "            else:\n",
    "                channels.append(None)\n",
    "        return np.array(list(zip(channels, DEEP_1010_CH_TYPES)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class To1020(InstanceTransform):\n",
    "\n",
    "    EEG_20_div = [\n",
    "               'FP1', 'FP2',\n",
    "        'F7', 'F3', 'FZ', 'F4', 'F8',\n",
    "        'T7', 'C3', 'CZ', 'C4', 'T8',\n",
    "        'T5', 'P3', 'PZ', 'P4', 'T6',\n",
    "                'O1', 'O2'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, only_trial_data=True, include_scale_ch=True, include_ref_chs=False):\n",
    "        \"\"\"\n",
    "        Transforms incoming Deep1010 data into exclusively the more limited 1020 channel set.\n",
    "        \"\"\"\n",
    "        super(To1020, self).__init__(only_trial_data=only_trial_data)\n",
    "        self._inds_20_div = [DEEP_1010_CHS_LISTING.index(ch) for ch in self.EEG_20_div]\n",
    "        if include_ref_chs:\n",
    "            self._inds_20_div.append([DEEP_1010_CHS_LISTING.index(ch) for ch in ['A1', 'A2']])\n",
    "        if include_scale_ch:\n",
    "            self._inds_20_div.append(SCALE_IND)\n",
    "\n",
    "    def new_channels(self, old_channels):\n",
    "        return old_channels[self._inds_20_div]\n",
    "\n",
    "    def __call__(self, *x):\n",
    "        x = list(x)\n",
    "        for i in range(len(x)):\n",
    "            # Assume every tensor that has deep1010 length should be modified\n",
    "            if len(x[i].shape) > 0 and x[i].shape[0] == len(DEEP_1010_CHS_LISTING):\n",
    "                x[i] = x[i][self._inds_20_div, ...]\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(x: torch.Tensor, low=-1, high=1):\n",
    "    if len(x.shape) == 2:\n",
    "        xmin = x.min()\n",
    "        xmax = x.max()\n",
    "        if xmax - xmin == 0:\n",
    "            x = 0\n",
    "            return x\n",
    "    elif len(x.shape) == 3:\n",
    "        xmin = torch.min(torch.min(x, keepdim=True, dim=1)[0], keepdim=True, dim=-1)[0]\n",
    "        xmax = torch.max(torch.max(x, keepdim=True, dim=1)[0], keepdim=True, dim=-1)[0]\n",
    "        constant_trials = (xmax - xmin) == 0\n",
    "        if torch.any(constant_trials):\n",
    "            # If normalizing multiple trials, stabilize the normalization\n",
    "            xmax[constant_trials] = xmax[constant_trials] + 1e-6\n",
    "\n",
    "    x = (x - xmin) / (xmax - xmin)\n",
    "\n",
    "    # Now all scaled 0 -> 1, remove 0.5 bias\n",
    "    x -= 0.5\n",
    "    # Adjust for low/high bias and scale up\n",
    "    x += (high + low) / 2\n",
    "    return (high - low) * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw eeg\n",
    "DATA_PATH_RAW = '../../data/eegmmidb (raw)/files/'\n",
    "\n",
    "patient = 'S001'\n",
    "run = 'R03'\n",
    "FILE = DATA_PATH_RAW+f'{patient}/{patient}{run}.edf'\n",
    "\n",
    "raw_full = get_raw(FILE)\n",
    "\n",
    "annotations = get_annotations(FILE)\n",
    "\n",
    "annotation_dict = get_window_dict(raw_full, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = annotation_dict['T0'][0]\n",
    "raw = pick_and_rename_channels(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "recording = RawTorchRecording(raw, 4 , stride=1, decimate=1, ch_ind_picks=None, bad_spans=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set param \n",
    "sfreq = 250\n",
    "new_sfreq = 256\n",
    "data_max = 3276.7\n",
    "data_min = -1583.9258304722666\n",
    "\n",
    "_dum = _DumbNamespace(dict(channels=recording.channels, info=dict(data_max=data_max, data_min=data_min)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xform = MappingDeep1010(_dum, return_mask = True)\n",
    "recording.add_transform(xform)\n",
    "\n",
    "recording.add_transform(To1020())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1020 = recording.__getitem__(0)[0]\n",
    "final_example = output1020[None, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 20, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5255, -0.4970, -0.4779,  ..., -0.6889, -0.7120, -0.7183],\n",
       "         [-0.5255, -0.5352, -0.5274,  ..., -0.7079, -0.7190, -0.7219],\n",
       "         [-0.5255, -0.5220, -0.5161,  ..., -0.5466, -0.5705, -0.5705],\n",
       "         ...,\n",
       "         [-0.5255, -0.5404, -0.5507,  ..., -0.5279, -0.5082, -0.4976],\n",
       "         [-0.5255, -0.5751, -0.6067,  ..., -0.5267, -0.5119, -0.4942],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5255, -0.4970, -0.4779,  ..., -0.6889, -0.7120, -0.7183],\n",
       "         [-0.5255, -0.5352, -0.5274,  ..., -0.7079, -0.7190, -0.7219],\n",
       "         [-0.5255, -0.5220, -0.5161,  ..., -0.5466, -0.5705, -0.5705],\n",
       "         ...,\n",
       "         [-0.5255, -0.5404, -0.5507,  ..., -0.5279, -0.5082, -0.4976],\n",
       "         [-0.5255, -0.5751, -0.6067,  ..., -0.5267, -0.5119, -0.4942],\n",
       "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMBER_CHANNELS = 20\n",
    "NUMBER_SAMPLES = 1024\n",
    "\n",
    "x = torch.zeros((1, NUMBER_CHANNELS, NUMBER_SAMPLES))\n",
    "\n",
    "x[:, :19, :] = torch.from_numpy(raw.copy().get_data()[:, :NUMBER_SAMPLES].reshape(1, NUMBER_CHANNELS - 1, NUMBER_SAMPLES))\n",
    "x = min_max_normalize(x)\n",
    "\n",
    "x[:,19,:] = torch.ones((1, NUMBER_SAMPLES)) * -1  \n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(final_example[:,:-1] == x[:,:-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
